{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **The Art of Chunking**\n",
    "\n",
    "> Reference: https://towardsdatascience.com/the-art-of-chunking-boosting-ai-performance-in-rag-architectures-acdbdb8bdc2b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Techniques to Improve Chunking**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Fixed Character Sizes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros:\n",
    "- **Simplicity**: Easy to implement and requires minimal computational resources.\n",
    "- **Consistency**: Produces uniform chunks, simplifying downstream processing.\n",
    "#### Cons:\n",
    "- **Context Ignorance**: Ignores the structure and meaning of the text, resulting in fragmented information.\n",
    "- **Inefficiency**: May cut off important context, requiring additional processing to reassemble meaningful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is the text I would like to ch', 'unk up. It is the example text for ', 'this exercise.']\n"
     ]
    }
   ],
   "source": [
    "# Sample text to chunk\n",
    "text = \"This is the text I would like to chunk up. It is the example text for this exercise.\"\n",
    "\n",
    "# Set the chunk size\n",
    "chunk_size = 35\n",
    "# Initialize a list to hold the chunks\n",
    "chunks = []\n",
    "# Iterate over the text to create chunks\n",
    "for i in range(0, len(text), chunk_size):\n",
    "    chunk = text[i:i + chunk_size]\n",
    "    chunks.append(chunk)\n",
    "# Display the chunks\n",
    "print(chunks)\n",
    "# Output: ['This is the text I would like to ch', 'unk up. It is the example text for ', 'this exercise']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using LangChain’s CharacterTextSplitter to achieve the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the text I would like to ch\n",
      "unk up. It is the example text for \n",
      "this exercise.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Initialize the text splitter with specified chunk size\n",
    "text_splitter = CharacterTextSplitter(chunk_size=35, chunk_overlap=0, separator='', strip_whitespace=False)\n",
    "# Create documents using the text splitter\n",
    "documents = text_splitter.create_documents([text])\n",
    "# Display the created documents\n",
    "for doc in documents:\n",
    "    print(doc.page_content)\n",
    "# Output: \n",
    "# This is the text I would like to ch\n",
    "# unk up. It is the example text for \n",
    "# this exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Recursive Character Chunking**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros:\n",
    "- **Improved Context**: This method preserves the text’s natural structure using separators like paragraphs or sentences.\n",
    "- **Flexibility**: Allows for varying chunk sizes and overlaps, providing better control over the chunking process.\n",
    "#### Cons:\n",
    "- **The chunk size matters**: It should be manageable but still contain at least one phrase or more. Otherwise, we need to gain precision while retrieving the chunk.\n",
    "- **Performance Overhead**: Requires more computational resources due to recursive splitting and handling of multiple separators. And we generate more chunks compared to fixed-size chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Olympic Games, originally\n",
      "Games, originally held in\n",
      "originally held in ancient\n",
      "held in ancient Greece, were\n",
      "Greece, were revived in 1896\n",
      "revived in 1896 and\n",
      "have since become the world’s\n",
      "become the world’s foremost\n",
      "world’s foremost sports\n",
      "foremost sports competition,\n",
      "sports competition, bringing\n",
      "bringing together\n",
      "athletes from around the\n",
      "from around the globe.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# Sample text to chunk\n",
    "text = \"\"\"\n",
    "The Olympic Games, originally held in ancient Greece, were revived in 1896 and\n",
    "have since become the world’s foremost sports competition, bringing together \n",
    "athletes from around the globe.\n",
    "\"\"\"\n",
    "# Initialize the recursive character text splitter with specified chunk size\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=30,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "# Create documents using the text splitter\n",
    "documents = text_splitter.create_documents([text])\n",
    "# Display the created documents\n",
    "for doc in documents:\n",
    "    print(doc.page_content)\n",
    "# Output:\n",
    "# “The Olympic Games, originally”\n",
    "# “held in ancient Greece, were”\n",
    "# “revived in 1896 and have”\n",
    "# “have since become the world’s”\n",
    "# “world’s foremost sports”\n",
    "# “competition, bringing together”\n",
    "# “together athletes from around”\n",
    "# “around the globe.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Document-Specific Splitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros:\n",
    "- **Relevance**: Different document types are split using the most appropriate method, preserving their logical structure.\n",
    "- **Precision**: Tailors the chunking process to the unique characteristics of each document type.\n",
    "#### Cons:\n",
    "- **Complex Implementation**: Requires different chunking strategies and libraries for various document types.\n",
    "- **Maintenance**: Maintenance is more complex due to the diversity of methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.1 Markdown Splitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Fun in California\n",
      "## Driving\n",
      "Try driving on the 1 down to San Diego\n",
      "### Food\n",
      "Make sure to eat a burrito while you're\n",
      "there\n",
      "## Hiking\n",
      "Go to Yosemite\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "# Sample Markdown text\n",
    "markdown_text = \"\"\"\n",
    "# Fun in California\n",
    "## Driving\n",
    "Try driving on the 1 down to San Diego\n",
    "### Food\n",
    "Make sure to eat a burrito while you're there\n",
    "## Hiking\n",
    "Go to Yosemite\n",
    "\"\"\"\n",
    "# Initialize the Markdown text splitter\n",
    "splitter = MarkdownTextSplitter(chunk_size=40, chunk_overlap=0)\n",
    "# Create documents using the text splitter\n",
    "documents = splitter.create_documents([markdown_text])\n",
    "# Display the created documents\n",
    "for doc in documents:\n",
    "    print(doc.page_content)\n",
    "# Output:\n",
    "# # Fun in California\\n\\n## Driving\n",
    "# Try driving on the 1 down to San Diego\n",
    "# ### Food\n",
    "# Make sure to eat a burrito while you're\n",
    "# there\n",
    "# ## Hiking\\n\\nGo to Yosemite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2 Python Code Splitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Person:\n",
      "    def __init__(self, name, age):\n",
      "        self.name = name\n",
      "        self.age = age\n",
      "p1 = Person(\"John\", 36)\n",
      "for i in range(10):\n",
      "    print(i)\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import PythonCodeTextSplitter\n",
    "# Sample Python code\n",
    "python_text = \"\"\"\n",
    "class Person:\n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "p1 = Person(\"John\", 36)\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "\"\"\"\n",
    "# Initialize the Python code text splitter\n",
    "python_splitter = PythonCodeTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "# Create documents using the text splitter\n",
    "documents = python_splitter.create_documents([python_text])\n",
    "# Display the created documents\n",
    "for doc in documents:\n",
    "    print(doc.page_content)\n",
    "# Output:\n",
    "# class Person:\\n    def __init__(self, name, age):\\n        self.name = name\\n        self.age = age\n",
    "# p1 = Person(\"John\", 36)\\n\\nfor i in range(10):\\n    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Semantic Splitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros:\n",
    "- **Contextual Relevance**: Ensures that chunks contain semantically similar content, enhancing the accuracy of information retrieval and generation.\n",
    "- **Dynamic Adaptability**: Can adapt to various text structures and content types based on meaning rather than rigid rules.\n",
    "#### Cons:\n",
    "- **Computational Overhead**: Requires additional computational resources to generate and compare embeddings.\n",
    "- **Complexity**: More complex to implement compared to simpler splitting methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reference: https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44830/1908490135.py:28: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  oai_embeds = OpenAIEmbeddings()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk #1:\n",
      "\n",
      "One of the most important things I didn't understand about the world when I was a child is the degree to which the returns for performance are superlinear. Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your product is only half as good as your competitor's, you don't get half as many customers.\n",
      "\n",
      "Chunk #2:\n",
      "You get no customers, and you go out of business. It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import re\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "One of the most important things I didn't understand about the world when I was a child is the degree to which the returns for performance are superlinear.\n",
    "Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your product is only half as good as your competitor's, you don't get half as many customers. You get no customers, and you go out of business.\n",
    "It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer.\n",
    "\"\"\"\n",
    "# Splitting the text into sentences\n",
    "sentences = re.split(r'(?<=[.?!])\\s+', text)\n",
    "sentences = [{'sentence': x, 'index' : i} for i, x in enumerate(sentences)]\n",
    "# Combine sentences for context\n",
    "def combine_sentences(sentences, buffer_size=1):\n",
    "    for i in range(len(sentences)):\n",
    "        combined_sentence = ''\n",
    "        for j in range(i - buffer_size, i):\n",
    "            if j >= 0:\n",
    "                combined_sentence += sentences[j]['sentence'] + ' '\n",
    "        combined_sentence += sentences[i]['sentence']\n",
    "        for j in range(i + 1, i + 1 + buffer_size):\n",
    "            if j < len(sentences):\n",
    "                combined_sentence += ' ' + sentences[j]['sentence']\n",
    "        sentences[i]['combined_sentence'] = combined_sentence\n",
    "    return sentences\n",
    "sentences = combine_sentences(sentences)\n",
    "# Generate embeddings\n",
    "oai_embeds = OpenAIEmbeddings()\n",
    "embeddings = oai_embeds.embed_documents([x['combined_sentence'] for x in sentences])\n",
    "# Add embeddings to sentences\n",
    "for i, sentence in enumerate(sentences):\n",
    "    sentence['combined_sentence_embedding'] = embeddings[i]\n",
    "# Calculate cosine distances\n",
    "def calculate_cosine_distances(sentences):\n",
    "    distances = []\n",
    "    for i in range(len(sentences) - 1):\n",
    "        embedding_current = sentences[i]['combined_sentence_embedding']\n",
    "        embedding_next = sentences[i + 1]['combined_sentence_embedding']\n",
    "        similarity = cosine_similarity([embedding_current], [embedding_next])[0][0]\n",
    "        distance = 1 - similarity\n",
    "        distances.append(distance)\n",
    "        sentences[i]['distance_to_next'] = distance\n",
    "    return distances, sentences\n",
    "distances, sentences = calculate_cosine_distances(sentences)\n",
    "# Determine breakpoints and create chunks\n",
    "import numpy as np\n",
    "breakpoint_distance_threshold = np.percentile(distances, 95)\n",
    "indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold]\n",
    "# Combine sentences into chunks\n",
    "chunks = []\n",
    "start_index = 0\n",
    "for index in indices_above_thresh:\n",
    "    end_index = index\n",
    "    group = sentences[start_index:end_index + 1]\n",
    "    combined_text = ' '.join([d['sentence'] for d in group])\n",
    "    chunks.append(combined_text)\n",
    "    start_index = index + 1\n",
    "if start_index < len(sentences):\n",
    "    combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])\n",
    "    chunks.append(combined_text)\n",
    "# Display the created chunks\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk #{i+1}:\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Agentic Splitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros:\n",
    "- **High Precision**: Provides highly relevant and contextually accurate chunks by using sophisticated language models.\n",
    "- **Adaptability**: Can handle diverse types of text and adjust chunking strategies on the fly.\n",
    "#### Cons:\n",
    "- **Resource Intensive and Additional LLM cost**: Requires significant computational resources to run large language models.\n",
    "- **Complex Implementation**: Involves setting up and fine-tuning language models for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'InputNode' from 'langgraph.prebuilt' (/root/anaconda3/envs/loki-311/lib/python3.11/site-packages/langgraph/prebuilt/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprebuilt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InputNode, SentenceSplitterNode, LLMDecisionNode, ChunkingNode\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Step 1: Input Node\u001b[39;00m\n\u001b[1;32m      4\u001b[0m input_node \u001b[38;5;241m=\u001b[39m InputNode(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocument Input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'InputNode' from 'langgraph.prebuilt' (/root/anaconda3/envs/loki-311/lib/python3.11/site-packages/langgraph/prebuilt/__init__.py)"
     ]
    }
   ],
   "source": [
    "# from langgraph.nodes import InputNode, SentenceSplitterNode, LLMDecisionNode, ChunkingNode\n",
    "\n",
    "# # Step 1: Input Node\n",
    "# input_node = InputNode(name=\"Document Input\")\n",
    "\n",
    "# # Step 2: Sentence Splitting Node\n",
    "# splitter_node = SentenceSplitterNode(input=input_node.output, name=\"Sentence Splitter\")\n",
    "\n",
    "# # Step 3: LLM Decision Node\n",
    "# decision_node = LLMDecisionNode(\n",
    "#     input=splitter_node.output, \n",
    "#     prompt_template=\"Does the sentence '{next_sentence}' belong to the same chunk as '{current_chunk}'?\", \n",
    "#     name=\"LLM Decision\"\n",
    "# )\n",
    "\n",
    "# # Step 4: Chunking Node\n",
    "# chunking_node = ChunkingNode(input=decision_node.output, name=\"Semantic Chunking\")\n",
    "\n",
    "# # Run the graph\n",
    "# document = \"Your document text here...\"\n",
    "# result = chunking_node.run(document=document)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/loki-311/lib/python3.11/site-packages/langsmith/client.py:333: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences=['On July 20, 1969, astronaut Neil Armstrong made history by becoming the first human to walk on the moon', 'He was the commander of NASA\\'s Apollo 11 mission, which was a monumental achievement in space exploration and a significant milestone in the Space Race between the United States and the Soviet Union.\\n\\nAs Armstrong descended from the lunar module, named Eagle, and set foot on the moon\\'s surface, he uttered the now-iconic words, \"That\\'s one small step for man, one giant leap for mankind.\" This statement encapsulated the profound significance of the event, emphasizing both the individual achievement and its broader implications for humanity.\\n\\nThe Apollo 11 mission not only demonstrated the technological prowess and determination of NASA but also served as a unifying moment that captured the imagination and aspirations of people around the world', 'Alongside Armstrong, astronaut Edwin \"Buzz\" Aldrin also walked on the moon, while Michael Collins piloted the command module, Columbia, in lunar orbit.\\n\\nThe success of Apollo 11 paved the way for subsequent lunar missions and cemented its place as a defining moment in the history of space exploration.']\n"
     ]
    }
   ],
   "source": [
    "# Pull the object from the hub\n",
    "obj = hub.pull(\"wfh/proposal-indexing\")\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# A Pydantic model to extract sentences from the passage\n",
    "class Sentences(BaseModel):\n",
    "    sentences: List[str]\n",
    "\n",
    "# Create the sentence extraction function\n",
    "def extract_sentences(text):\n",
    "    # Get the response from the LLM\n",
    "    response = llm.invoke(text)\n",
    "    \n",
    "    # Extract the content of the AIMessage object\n",
    "    response_text = response.content\n",
    "    \n",
    "    # Split the response text into sentences\n",
    "    sentences = response_text.split('. ')\n",
    "    \n",
    "    # Create and return the structured output using the Pydantic model\n",
    "    return Sentences(sentences=sentences)\n",
    "\n",
    "# Test it out\n",
    "text = \"\"\"\n",
    "On July 20, 1969, astronaut Neil Armstrong walked on the moon. \n",
    "He was leading NASA's Apollo 11 mission. \n",
    "Armstrong famously said, \"That's one small step for man, one giant leap for mankind\" as he stepped onto the lunar surface.\n",
    "\"\"\"\n",
    "\n",
    "# Process and validate the extracted sentences\n",
    "sentences = extract_sentences(text)\n",
    "\n",
    "# Print the extracted sentences\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "chunks = {}\n",
    "\n",
    "def create_new_chunk(chunk_id, proposition):\n",
    "    summary_llm = llm.with_structured_output(ChunkMeta)\n",
    "\n",
    "    summary_prompt_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"Generate a new summary and a title based on the propositions.\",\n",
    "            ),\n",
    "            (\n",
    "                \"user\",\n",
    "                \"propositions:{propositions}\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    summary_chain = summary_prompt_template | summary_llm\n",
    "\n",
    "    chunk_meta = summary_chain.invoke(\n",
    "        {\n",
    "            \"propositions\": [proposition],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    chunks[chunk_id] = {\n",
    "        \"summary\": chunk_meta.summary,\n",
    "        \"title\": chunk_meta.title,\n",
    "        \"propositions\": [proposition],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class ChunkMeta(BaseModel):\n",
    "    title: str = Field(description=\"The title of the chunk.\")\n",
    "    summary: str = Field(description=\"The summary of the chunk.\")\n",
    "\n",
    "def add_proposition(chunk_id, proposition):\n",
    "    summary_llm = llm.with_structured_output(ChunkMeta)\n",
    "\n",
    "    summary_prompt_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"If the current_summary and title is still valid for the propositions return them.\"\n",
    "                \"If not generate a new summary and a title based on the propositions.\",\n",
    "            ),\n",
    "            (\n",
    "                \"user\",\n",
    "                \"current_summary:{current_summary}\\n\\ncurrent_title:{current_title}\\n\\npropositions:{propositions}\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    summary_chain = summary_prompt_template | summary_llm\n",
    "\n",
    "    chunk = chunks[chunk_id]\n",
    "\n",
    "    current_summary = chunk[\"summary\"]\n",
    "    current_title = chunk[\"title\"]\n",
    "    current_propositions = chunk[\"propositions\"]\n",
    "\n",
    "    all_propositions = current_propositions + [proposition]\n",
    "\n",
    "    chunk_meta = summary_chain.invoke(\n",
    "        {\n",
    "            \"current_summary\": current_summary,\n",
    "            \"current_title\": current_title,\n",
    "            \"propositions\": all_propositions,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    chunk[\"summary\"] = chunk_meta.summary\n",
    "    chunk[\"title\"] = chunk_meta.title\n",
    "    chunk[\"propositions\"] = all_propositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_chunk_and_push_proposition(proposition):\n",
    "\n",
    "    class ChunkID(BaseModel):\n",
    "        chunk_id: int = Field(description=\"The chunk id.\")\n",
    "\n",
    "    allocation_llm = llm.with_structured_output(ChunkID)\n",
    "\n",
    "    allocation_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You have the chunk ids and the summaries\"\n",
    "                \"Find the chunk that best matches the proposition.\"\n",
    "                \"If no chunk matches, return a new chunk id.\"\n",
    "                \"Return only the chunk id.\",\n",
    "            ),\n",
    "            (\n",
    "                \"user\",\n",
    "                \"proposition:{proposition}\" \"chunks_summaries:{chunks_summaries}\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    allocation_chain = allocation_prompt | allocation_llm\n",
    "\n",
    "    chunks_summaries = {\n",
    "        chunk_id: chunk[\"summary\"] for chunk_id, chunk in chunks.items()\n",
    "    }\n",
    "\n",
    "    best_chunk_id = allocation_chain.invoke(\n",
    "        {\"proposition\": proposition, \"chunks_summaries\": chunks_summaries}\n",
    "    ).chunk_id\n",
    "\n",
    "    if best_chunk_id not in chunks:\n",
    "        best_chunk_id = create_new_chunk(best_chunk_id, proposition)\n",
    "        return\n",
    "\n",
    "    add_proposition(best_chunk_id, proposition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reference: https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/d2da20552446179779c74ccc9e232e77ba981659/5_Levels_Of_Text_Splitting.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.output_parsers.openai_tools import JsonOutputToolsParser\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.chains import create_extraction_chain\n",
    "from typing import Optional, List\n",
    "from langchain.chains import create_extraction_chain_pydantic\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/loki-311/lib/python3.11/site-packages/langsmith/client.py:333: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/loki-311/lib/python3.11/site-packages/langsmith/client.py:5434: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  prompt = loads(json.dumps(prompt_object.manifest))\n",
      "/tmp/ipykernel_59433/3277564191.py:2: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  llm = ChatOpenAI(model='gpt-4o', openai_api_key = os.getenv(\"OPENAI_API_KEY\"))\n"
     ]
    }
   ],
   "source": [
    "obj = hub.pull(\"wfh/proposal-indexing\")\n",
    "llm = ChatOpenAI(model='gpt-4o', openai_api_key = os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use it in a runnable\n",
    "runnable = obj | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59433/2581963727.py:6: LangChainDeprecationWarning: LangChain has introduced a method called `with_structured_output` thatis available on ChatModels capable of tool calling.You can read more about the method here: <https://python.langchain.com/docs/modules/model_io/chat/structured_output/>. Please follow our extraction use case documentation for more guidelineson how to do information extraction with LLMs.<https://python.langchain.com/docs/use_cases/extraction/>. If you notice other issues, please provide feedback here:<https://github.com/langchain-ai/langchain/discussions/18154>\n",
      "  extraction_chain = create_extraction_chain_pydantic(pydantic_schema=Sentences, llm=llm)\n"
     ]
    }
   ],
   "source": [
    "# Pydantic data class\n",
    "class Sentences(BaseModel):\n",
    "    sentences: List[str]\n",
    "    \n",
    "# Extraction\n",
    "extraction_chain = create_extraction_chain_pydantic(pydantic_schema=Sentences, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_propositions(text):\n",
    "    runnable_output = runnable.invoke({\n",
    "    \t\"input\": text\n",
    "    }).content\n",
    "    \n",
    "    propositions = extraction_chain.run(runnable_output)[0].sentences\n",
    "    return propositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay =  \"\"\"\n",
    "On July 20, 1969, astronaut Neil Armstrong walked on the moon. \n",
    "He was leading NASA's Apollo 11 mission. \n",
    "Armstrong famously said, \"That's one small step for man, one giant leap for mankind\" as he stepped onto the lunar surface.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = essay.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59433/4174401463.py:6: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  propositions = extraction_chain.run(runnable_output)[0].sentences\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 0\n",
      "Done with 1\n",
      "Done with 2\n",
      "Done with 3\n",
      "Done with 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "essay_propositions = []\n",
    "\n",
    "for i, para in enumerate(paragraphs):\n",
    "    propositions = get_propositions(para)\n",
    "    \n",
    "    essay_propositions.extend(propositions)\n",
    "    print (f\"Done with {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 7 propositions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Sure, please provide the content you would like me to decompose.',\n",
       " 'On July 20, 1969, Neil Armstrong walked on the moon.',\n",
       " 'Neil Armstrong was an astronaut.',\n",
       " \"He was leading NASA's Apollo 11 mission.\",\n",
       " 'Neil Armstrong famously said a quote when he stepped onto the lunar surface.',\n",
       " \"Neil Armstrong said, 'That's one small step for man, one giant leap for mankind.'\",\n",
       " 'Sure, please provide the content you would like to have decomposed.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print (f\"You have {len(essay_propositions)} propositions\")\n",
    "essay_propositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini script I made\n",
    "from rag.medium.agentic_chunker import AgenticChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = AgenticChunker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding: 'Sure, please provide the content you would like me to decompose.'\n",
      "No chunks, creating a new one\n",
      "Created new chunk (1921f): Content Analysis Requests\n",
      "\n",
      "Adding: 'On July 20, 1969, Neil Armstrong walked on the moon.'\n",
      "No chunks found\n",
      "Created new chunk (56ed1): Space Exploration History\n",
      "\n",
      "Adding: 'Neil Armstrong was an astronaut.'\n",
      "Chunk Found (56ed1), adding to: Space Exploration History\n",
      "\n",
      "Adding: 'He was leading NASA's Apollo 11 mission.'\n",
      "Chunk Found (56ed1), adding to: Neil Armstrong & Moon Landing\n",
      "\n",
      "Adding: 'Neil Armstrong famously said a quote when he stepped onto the lunar surface.'\n",
      "Chunk Found (56ed1), adding to: Neil Armstrong's Astronaut Career & Historic Achievements\n",
      "\n",
      "Adding: 'Neil Armstrong said, 'That's one small step for man, one giant leap for mankind.''\n",
      "Chunk Found (56ed1), adding to: Neil Armstrong & Apollo 11 Moon Landing\n",
      "\n",
      "Adding: 'Sure, please provide the content you would like to have decomposed.'\n",
      "Chunk Found (1921f), adding to: Content Analysis Requests\n"
     ]
    }
   ],
   "source": [
    "ac.add_propositions(essay_propositions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You have 2 chunks\n",
      "\n",
      "Chunk #0\n",
      "Chunk ID: 1921f\n",
      "Summary: This chunk contains conversations about requests for content decomposition or analysis.\n",
      "Propositions:\n",
      "    -Sure, please provide the content you would like me to decompose.\n",
      "    -Sure, please provide the content you would like to have decomposed.\n",
      "\n",
      "\n",
      "\n",
      "Chunk #1\n",
      "Chunk ID: 56ed1\n",
      "Summary: This chunk contains information about Neil Armstrong's life, his career with NASA, details of the Apollo 11 mission, and his iconic quote from the moon landing.\n",
      "Propositions:\n",
      "    -On July 20, 1969, Neil Armstrong walked on the moon.\n",
      "    -Neil Armstrong was an astronaut.\n",
      "    -He was leading NASA's Apollo 11 mission.\n",
      "    -Neil Armstrong famously said a quote when he stepped onto the lunar surface.\n",
      "    -Neil Armstrong said, 'That's one small step for man, one giant leap for mankind.'\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ac.pretty_print_chunks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sure, please provide the content you would like me to decompose. Sure, please provide the content you would like to have decomposed.',\n",
       " \"On July 20, 1969, Neil Armstrong walked on the moon. Neil Armstrong was an astronaut. He was leading NASA's Apollo 11 mission. Neil Armstrong famously said a quote when he stepped onto the lunar surface. Neil Armstrong said, 'That's one small step for man, one giant leap for mankind.'\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = ac.get_chunks(get_type='list_of_strings')\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
