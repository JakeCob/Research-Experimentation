{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **The Art of Chunking**\n",
    "\n",
    "> Reference: https://towardsdatascience.com/the-art-of-chunking-boosting-ai-performance-in-rag-architectures-acdbdb8bdc2b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Techniques to Improve Chunking**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Fixed Character Sizes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros:\n",
    "- **Simplicity**: Easy to implement and requires minimal computational resources.\n",
    "- **Consistency**: Produces uniform chunks, simplifying downstream processing.\n",
    "#### Cons:\n",
    "- **Context Ignorance**: Ignores the structure and meaning of the text, resulting in fragmented information.\n",
    "- **Inefficiency**: May cut off important context, requiring additional processing to reassemble meaningful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is the text I would like to ch', 'unk up. It is the example text for ', 'this exercise.']\n"
     ]
    }
   ],
   "source": [
    "# Sample text to chunk\n",
    "text = \"This is the text I would like to chunk up. It is the example text for this exercise.\"\n",
    "\n",
    "# Set the chunk size\n",
    "chunk_size = 35\n",
    "# Initialize a list to hold the chunks\n",
    "chunks = []\n",
    "# Iterate over the text to create chunks\n",
    "for i in range(0, len(text), chunk_size):\n",
    "    chunk = text[i:i + chunk_size]\n",
    "    chunks.append(chunk)\n",
    "# Display the chunks\n",
    "print(chunks)\n",
    "# Output: ['This is the text I would like to ch', 'unk up. It is the example text for ', 'this exercise']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using LangChain’s CharacterTextSplitter to achieve the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the text I would like to ch\n",
      "unk up. It is the example text for \n",
      "this exercise.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Initialize the text splitter with specified chunk size\n",
    "text_splitter = CharacterTextSplitter(chunk_size=35, chunk_overlap=0, separator='', strip_whitespace=False)\n",
    "# Create documents using the text splitter\n",
    "documents = text_splitter.create_documents([text])\n",
    "# Display the created documents\n",
    "for doc in documents:\n",
    "    print(doc.page_content)\n",
    "# Output: \n",
    "# This is the text I would like to ch\n",
    "# unk up. It is the example text for \n",
    "# this exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Recursive Character Chunking**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros:\n",
    "- **Improved Context**: This method preserves the text’s natural structure using separators like paragraphs or sentences.\n",
    "- **Flexibility**: Allows for varying chunk sizes and overlaps, providing better control over the chunking process.\n",
    "#### Cons:\n",
    "- **The chunk size matters**: It should be manageable but still contain at least one phrase or more. Otherwise, we need to gain precision while retrieving the chunk.\n",
    "- **Performance Overhead**: Requires more computational resources due to recursive splitting and handling of multiple separators. And we generate more chunks compared to fixed-size chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Olympic Games, originally\n",
      "Games, originally held in\n",
      "originally held in ancient\n",
      "held in ancient Greece, were\n",
      "Greece, were revived in 1896\n",
      "revived in 1896 and\n",
      "have since become the world’s\n",
      "become the world’s foremost\n",
      "world’s foremost sports\n",
      "foremost sports competition,\n",
      "sports competition, bringing\n",
      "bringing together\n",
      "athletes from around the\n",
      "from around the globe.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# Sample text to chunk\n",
    "text = \"\"\"\n",
    "The Olympic Games, originally held in ancient Greece, were revived in 1896 and\n",
    "have since become the world’s foremost sports competition, bringing together \n",
    "athletes from around the globe.\n",
    "\"\"\"\n",
    "# Initialize the recursive character text splitter with specified chunk size\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=30,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "# Create documents using the text splitter\n",
    "documents = text_splitter.create_documents([text])\n",
    "# Display the created documents\n",
    "for doc in documents:\n",
    "    print(doc.page_content)\n",
    "# Output:\n",
    "# “The Olympic Games, originally”\n",
    "# “held in ancient Greece, were”\n",
    "# “revived in 1896 and have”\n",
    "# “have since become the world’s”\n",
    "# “world’s foremost sports”\n",
    "# “competition, bringing together”\n",
    "# “together athletes from around”\n",
    "# “around the globe.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Document-Specific Splitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros:\n",
    "- **Relevance**: Different document types are split using the most appropriate method, preserving their logical structure.\n",
    "- **Precision**: Tailors the chunking process to the unique characteristics of each document type.\n",
    "#### Cons:\n",
    "- **Complex Implementation**: Requires different chunking strategies and libraries for various document types.\n",
    "- **Maintenance**: Maintenance is more complex due to the diversity of methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.1 Markdown Splitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Fun in California\n",
      "## Driving\n",
      "Try driving on the 1 down to San Diego\n",
      "### Food\n",
      "Make sure to eat a burrito while you're\n",
      "there\n",
      "## Hiking\n",
      "Go to Yosemite\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "# Sample Markdown text\n",
    "markdown_text = \"\"\"\n",
    "# Fun in California\n",
    "## Driving\n",
    "Try driving on the 1 down to San Diego\n",
    "### Food\n",
    "Make sure to eat a burrito while you're there\n",
    "## Hiking\n",
    "Go to Yosemite\n",
    "\"\"\"\n",
    "# Initialize the Markdown text splitter\n",
    "splitter = MarkdownTextSplitter(chunk_size=40, chunk_overlap=0)\n",
    "# Create documents using the text splitter\n",
    "documents = splitter.create_documents([markdown_text])\n",
    "# Display the created documents\n",
    "for doc in documents:\n",
    "    print(doc.page_content)\n",
    "# Output:\n",
    "# # Fun in California\\n\\n## Driving\n",
    "# Try driving on the 1 down to San Diego\n",
    "# ### Food\n",
    "# Make sure to eat a burrito while you're\n",
    "# there\n",
    "# ## Hiking\\n\\nGo to Yosemite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2 Python Code Splitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Person:\n",
      "    def __init__(self, name, age):\n",
      "        self.name = name\n",
      "        self.age = age\n",
      "p1 = Person(\"John\", 36)\n",
      "for i in range(10):\n",
      "    print(i)\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import PythonCodeTextSplitter\n",
    "# Sample Python code\n",
    "python_text = \"\"\"\n",
    "class Person:\n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "p1 = Person(\"John\", 36)\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "\"\"\"\n",
    "# Initialize the Python code text splitter\n",
    "python_splitter = PythonCodeTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "# Create documents using the text splitter\n",
    "documents = python_splitter.create_documents([python_text])\n",
    "# Display the created documents\n",
    "for doc in documents:\n",
    "    print(doc.page_content)\n",
    "# Output:\n",
    "# class Person:\\n    def __init__(self, name, age):\\n        self.name = name\\n        self.age = age\n",
    "# p1 = Person(\"John\", 36)\\n\\nfor i in range(10):\\n    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Semantic Splitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros:\n",
    "- **Contextual Relevance**: Ensures that chunks contain semantically similar content, enhancing the accuracy of information retrieval and generation.\n",
    "- **Dynamic Adaptability**: Can adapt to various text structures and content types based on meaning rather than rigid rules.\n",
    "#### Cons:\n",
    "- **Computational Overhead**: Requires additional computational resources to generate and compare embeddings.\n",
    "- **Complexity**: More complex to implement compared to simpler splitting methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reference: https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44830/1908490135.py:28: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  oai_embeds = OpenAIEmbeddings()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk #1:\n",
      "\n",
      "One of the most important things I didn't understand about the world when I was a child is the degree to which the returns for performance are superlinear. Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your product is only half as good as your competitor's, you don't get half as many customers.\n",
      "\n",
      "Chunk #2:\n",
      "You get no customers, and you go out of business. It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import re\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "One of the most important things I didn't understand about the world when I was a child is the degree to which the returns for performance are superlinear.\n",
    "Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your product is only half as good as your competitor's, you don't get half as many customers. You get no customers, and you go out of business.\n",
    "It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer.\n",
    "\"\"\"\n",
    "# Splitting the text into sentences\n",
    "sentences = re.split(r'(?<=[.?!])\\s+', text)\n",
    "sentences = [{'sentence': x, 'index' : i} for i, x in enumerate(sentences)]\n",
    "# Combine sentences for context\n",
    "def combine_sentences(sentences, buffer_size=1):\n",
    "    for i in range(len(sentences)):\n",
    "        combined_sentence = ''\n",
    "        for j in range(i - buffer_size, i):\n",
    "            if j >= 0:\n",
    "                combined_sentence += sentences[j]['sentence'] + ' '\n",
    "        combined_sentence += sentences[i]['sentence']\n",
    "        for j in range(i + 1, i + 1 + buffer_size):\n",
    "            if j < len(sentences):\n",
    "                combined_sentence += ' ' + sentences[j]['sentence']\n",
    "        sentences[i]['combined_sentence'] = combined_sentence\n",
    "    return sentences\n",
    "sentences = combine_sentences(sentences)\n",
    "# Generate embeddings\n",
    "oai_embeds = OpenAIEmbeddings()\n",
    "embeddings = oai_embeds.embed_documents([x['combined_sentence'] for x in sentences])\n",
    "# Add embeddings to sentences\n",
    "for i, sentence in enumerate(sentences):\n",
    "    sentence['combined_sentence_embedding'] = embeddings[i]\n",
    "# Calculate cosine distances\n",
    "def calculate_cosine_distances(sentences):\n",
    "    distances = []\n",
    "    for i in range(len(sentences) - 1):\n",
    "        embedding_current = sentences[i]['combined_sentence_embedding']\n",
    "        embedding_next = sentences[i + 1]['combined_sentence_embedding']\n",
    "        similarity = cosine_similarity([embedding_current], [embedding_next])[0][0]\n",
    "        distance = 1 - similarity\n",
    "        distances.append(distance)\n",
    "        sentences[i]['distance_to_next'] = distance\n",
    "    return distances, sentences\n",
    "distances, sentences = calculate_cosine_distances(sentences)\n",
    "# Determine breakpoints and create chunks\n",
    "import numpy as np\n",
    "breakpoint_distance_threshold = np.percentile(distances, 95)\n",
    "indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold]\n",
    "# Combine sentences into chunks\n",
    "chunks = []\n",
    "start_index = 0\n",
    "for index in indices_above_thresh:\n",
    "    end_index = index\n",
    "    group = sentences[start_index:end_index + 1]\n",
    "    combined_text = ' '.join([d['sentence'] for d in group])\n",
    "    chunks.append(combined_text)\n",
    "    start_index = index + 1\n",
    "if start_index < len(sentences):\n",
    "    combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])\n",
    "    chunks.append(combined_text)\n",
    "# Display the created chunks\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk #{i+1}:\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Agentic Splitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros:\n",
    "- **High Precision**: Provides highly relevant and contextually accurate chunks by using sophisticated language models.\n",
    "- **Adaptability**: Can handle diverse types of text and adjust chunking strategies on the fly.\n",
    "#### Cons:\n",
    "- **Resource Intensive and Additional LLM cost**: Requires significant computational resources to run large language models.\n",
    "- **Complex Implementation**: Involves setting up and fine-tuning language models for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'InputNode' from 'langgraph.prebuilt' (/root/anaconda3/envs/loki-311/lib/python3.11/site-packages/langgraph/prebuilt/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprebuilt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InputNode, SentenceSplitterNode, LLMDecisionNode, ChunkingNode\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Step 1: Input Node\u001b[39;00m\n\u001b[1;32m      4\u001b[0m input_node \u001b[38;5;241m=\u001b[39m InputNode(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocument Input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'InputNode' from 'langgraph.prebuilt' (/root/anaconda3/envs/loki-311/lib/python3.11/site-packages/langgraph/prebuilt/__init__.py)"
     ]
    }
   ],
   "source": [
    "from langgraph.nodes import InputNode, SentenceSplitterNode, LLMDecisionNode, ChunkingNode\n",
    "\n",
    "# Step 1: Input Node\n",
    "input_node = InputNode(name=\"Document Input\")\n",
    "\n",
    "# Step 2: Sentence Splitting Node\n",
    "splitter_node = SentenceSplitterNode(input=input_node.output, name=\"Sentence Splitter\")\n",
    "\n",
    "# Step 3: LLM Decision Node\n",
    "decision_node = LLMDecisionNode(\n",
    "    input=splitter_node.output, \n",
    "    prompt_template=\"Does the sentence '{next_sentence}' belong to the same chunk as '{current_chunk}'?\", \n",
    "    name=\"LLM Decision\"\n",
    ")\n",
    "\n",
    "# Step 4: Chunking Node\n",
    "chunking_node = ChunkingNode(input=decision_node.output, name=\"Semantic Chunking\")\n",
    "\n",
    "# Run the graph\n",
    "document = \"Your document text here...\"\n",
    "result = chunking_node.run(document=document)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: No module named 'langgraph.nodes'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from langgraph.core import InputNode, SentenceSplitterNode, LLMDecisionNode, ChunkingNode\n",
    "\n",
    "    # Step 1: Input Node\n",
    "    input_node = InputNode(name=\"Document Input\")\n",
    "\n",
    "    # Step 2: Sentence Splitting Node\n",
    "    splitter_node = SentenceSplitterNode(input=input_node.output, name=\"Sentence Splitter\")\n",
    "\n",
    "    # Step 3: LLM Decision Node\n",
    "    decision_node = LLMDecisionNode(\n",
    "        input=splitter_node.output, \n",
    "        prompt_template=\"Does the sentence '{next_sentence}' belong to the same chunk as '{current_chunk}'?\", \n",
    "        name=\"LLM Decision\"\n",
    "    )\n",
    "\n",
    "    # Step 4: Chunking Node\n",
    "    chunking_node = ChunkingNode(input=decision_node.output, name=\"Semantic Chunking\")\n",
    "\n",
    "    # Run the graph\n",
    "    document = \"Your document text here...\"\n",
    "    result = chunking_node.run(document=document)\n",
    "    print(result)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
